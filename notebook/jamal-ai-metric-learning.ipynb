{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# üß† JAMAL AI - Similarity Metric Learning untuk Brainstorming\n\n## Topik: Similarity Metric Learning\n**Mata Kuliah**: Kecerdasan Buatan\n\n### Tujuan\nMembangun model **Siamese Neural Network** dengan **Contrastive Loss** untuk mengukur kesamaan semantik antar ide/sticky notes di aplikasi brainstorming (seperti FigJam).\n\n### Algoritma Utama\n1. **Siamese Network** - Twin network dengan shared weights\n2. **Contrastive Loss** - Loss function untuk metric learning\n3. **Euclidean Distance** - Mengukur jarak di embedding space\n\n### Dataset\n- **STS Benchmark** (Semantic Textual Similarity) - Dataset standar untuk similarity tasks\n- **Custom Brainstorming Test Data** - Data pengujian domain-specific",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 1: SETUP ENVIRONMENT\n# =============================================================================\n\nimport os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers, Model, Input\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\nprint(\"‚úÖ Environment Ready!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 2: KONFIGURASI MODEL\n# =============================================================================\n\n# --- HYPERPARAMETERS ---\nMAX_VOCAB = 15000        # Ukuran vocabulary\nMAX_LEN = 50             # Panjang maksimum sequence (dinaikkan untuk STS)\nEMBEDDING_DIM = 128      # Dimensi embedding (dinaikkan)\nLSTM_UNITS = 128         # Units LSTM (dinaikkan)\nDENSE_UNITS = 64         # Output embedding dimension\nBATCH_SIZE = 64          # Batch size\nEPOCHS = 15              # Epoch training\nMARGIN = 1.0             # Margin untuk contrastive loss\n\nprint(\"üìã KONFIGURASI MODEL:\")\nprint(f\"   MAX_VOCAB: {MAX_VOCAB}\")\nprint(f\"   MAX_LEN: {MAX_LEN}\")\nprint(f\"   EMBEDDING_DIM: {EMBEDDING_DIM}\")\nprint(f\"   LSTM_UNITS: {LSTM_UNITS}\")\nprint(f\"   DENSE_UNITS: {DENSE_UNITS}\")\nprint(f\"   MARGIN: {MARGIN}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 3: LOAD STS BENCHMARK DATASET\n# =============================================================================\n\n# Install datasets library jika belum ada\n!pip install -q datasets\n\nfrom datasets import load_dataset\n\nprint(\"üîÑ Loading STS Benchmark Dataset...\")\n\n# Load STS Benchmark dari Hugging Face\ntry:\n    sts_train = load_dataset(\"mteb/stsbenchmark-sts\", split=\"train\")\n    sts_test = load_dataset(\"mteb/stsbenchmark-sts\", split=\"test\")\n    \n    print(f\"‚úÖ Train samples: {len(sts_train)}\")\n    print(f\"‚úÖ Test samples: {len(sts_test)}\")\n    \n    # Show sample\n    print(\"\\nüìù Sample Data:\")\n    for i in range(3):\n        sample = sts_train[i]\n        print(f\"   S1: {sample['sentence1']}\")\n        print(f\"   S2: {sample['sentence2']}\")\n        print(f\"   Score: {sample['score']:.2f}/5.0\")\n        print()\nexcept Exception as e:\n    print(f\"‚ùå Error loading dataset: {e}\")\n    print(\"   Pastikan Internet enabled di Kaggle Settings!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 4: PREPROCESSING DATA\n# =============================================================================\n\ndef prepare_sts_data(dataset, threshold=2.5):\n    \"\"\"\n    Konversi STS dataset (score 0-5) ke binary labels.\n    Score >= threshold = Similar (1)\n    Score < threshold = Different (0)\n    \"\"\"\n    sentences1 = []\n    sentences2 = []\n    labels = []\n    scores = []  # Keep original scores for evaluation\n    \n    for sample in dataset:\n        s1 = str(sample['sentence1']).strip()\n        s2 = str(sample['sentence2']).strip()\n        score = float(sample['score'])\n        \n        if s1 and s2:  # Skip empty\n            sentences1.append(s1)\n            sentences2.append(s2)\n            scores.append(score)\n            # Binary: 1 jika score >= threshold (mirip), 0 jika tidak\n            labels.append(1.0 if score >= threshold else 0.0)\n    \n    return np.array(sentences1), np.array(sentences2), np.array(labels), np.array(scores)\n\n# Prepare data\nprint(\"üîÑ Preparing training data...\")\ntrain_s1, train_s2, train_labels, train_scores = prepare_sts_data(sts_train)\ntest_s1, test_s2, test_labels, test_scores = prepare_sts_data(sts_test)\n\nprint(f\"\\nüìä DATA STATISTICS:\")\nprint(f\"   Training pairs: {len(train_labels)}\")\nprint(f\"   Test pairs: {len(test_labels)}\")\nprint(f\"   Train - Similar (1): {sum(train_labels):.0f} ({sum(train_labels)/len(train_labels)*100:.1f}%)\")\nprint(f\"   Train - Different (0): {len(train_labels)-sum(train_labels):.0f} ({(1-sum(train_labels)/len(train_labels))*100:.1f}%)\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 5: TOKENISASI\n# =============================================================================\n\nprint(\"üîÑ Tokenizing text...\")\n\n# Fit tokenizer pada semua teks\ntokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\nall_texts = list(train_s1) + list(train_s2) + list(test_s1) + list(test_s2)\ntokenizer.fit_on_texts(all_texts)\n\n# Convert to sequences\ntrain_seq1 = tokenizer.texts_to_sequences(train_s1)\ntrain_seq2 = tokenizer.texts_to_sequences(train_s2)\ntest_seq1 = tokenizer.texts_to_sequences(test_s1)\ntest_seq2 = tokenizer.texts_to_sequences(test_s2)\n\n# Padding\nX1_train = pad_sequences(train_seq1, maxlen=MAX_LEN, padding='post')\nX2_train = pad_sequences(train_seq2, maxlen=MAX_LEN, padding='post')\nX1_test = pad_sequences(test_seq1, maxlen=MAX_LEN, padding='post')\nX2_test = pad_sequences(test_seq2, maxlen=MAX_LEN, padding='post')\n\ny_train = train_labels\ny_test = test_labels\n\nprint(f\"‚úÖ Tokenization complete!\")\nprint(f\"   Vocabulary size: {min(len(tokenizer.word_index)+1, MAX_VOCAB)}\")\nprint(f\"   X1_train shape: {X1_train.shape}\")\nprint(f\"   X2_train shape: {X2_train.shape}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 6: SIAMESE NETWORK ARCHITECTURE\n# =============================================================================\n\n# --- EUCLIDEAN DISTANCE ---\ndef euclidean_distance(vectors):\n    \"\"\"\n    Menghitung Euclidean distance antara dua vektor embedding.\n    d(A, B) = sqrt(sum((A - B)^2))\n    \"\"\"\n    (featsA, featsB) = vectors\n    sum_squared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_squared, K.epsilon()))\n\n# --- CONTRASTIVE LOSS ---\ndef contrastive_loss(y_true, y_pred):\n    \"\"\"\n    Contrastive Loss Function untuk Metric Learning.\n    \n    Formula:\n    L = y * d^2 + (1 - y) * max(margin - d, 0)^2\n    \n    Dimana:\n    - y = 1 (similar): Loss = d^2 (dorong jarak ke 0)\n    - y = 0 (different): Loss = max(margin - d, 0)^2 (dorong jarak > margin)\n    \"\"\"\n    square_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(MARGIN - y_pred, 0))\n    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n\nprint(\"üìê Loss Function: Contrastive Loss\")\nprint(f\"   Margin: {MARGIN}\")\nprint(\"   - Similar pairs (y=1): minimize distance\")\nprint(\"   - Different pairs (y=0): push distance > margin\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 7: BUILD SIAMESE MODEL\n# =============================================================================\n\n# --- BASE NETWORK (Shared Weights) ---\ndef create_base_network():\n    \"\"\"\n    Base network yang akan di-share oleh kedua input.\n    Architecture: Embedding -> LSTM -> Dense\n    \"\"\"\n    input_seq = Input(shape=(MAX_LEN,), name='input_sequence')\n    \n    # Embedding layer\n    x = layers.Embedding(MAX_VOCAB, EMBEDDING_DIM, name='embedding')(input_seq)\n    \n    # Bidirectional LSTM untuk capture context dari dua arah\n    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=False), name='bilstm')(x)\n    \n    # Dropout untuk regularization\n    x = layers.Dropout(0.3)(x)\n    \n    # Dense layer untuk output embedding\n    x = layers.Dense(DENSE_UNITS, activation='relu', name='dense')(x)\n    \n    # L2 Normalize embedding (penting untuk metric learning)\n    x = layers.Lambda(lambda t: K.l2_normalize(t, axis=1), name='l2_norm')(x)\n    \n    return Model(input_seq, x, name='base_network')\n\n# Create base network\nbase_network = create_base_network()\nprint(\"üîß BASE NETWORK ARCHITECTURE:\")\nbase_network.summary()\n\n# --- SIAMESE MODEL ---\ninput_a = Input(shape=(MAX_LEN,), name='input_sentence_1')\ninput_b = Input(shape=(MAX_LEN,), name='input_sentence_2')\n\n# Share weights - kedua input menggunakan network yang sama\nembedding_a = base_network(input_a)\nembedding_b = base_network(input_b)\n\n# Compute distance\ndistance = layers.Lambda(euclidean_distance, name='euclidean_distance')([embedding_a, embedding_b])\n\n# Final model\nsiamese_model = Model(inputs=[input_a, input_b], outputs=distance, name='siamese_network')\nsiamese_model.compile(loss=contrastive_loss, optimizer='adam')\n\nprint(\"\\nüîß SIAMESE NETWORK ARCHITECTURE:\")\nsiamese_model.summary()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 8: TRAINING\n# =============================================================================\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Callbacks\nearly_stop = EarlyStopping(\n    monitor='val_loss', \n    patience=3, \n    verbose=1,\n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=2,\n    verbose=1,\n    min_lr=1e-6\n)\n\ncheckpoint = ModelCheckpoint(\n    'jamal_metric_learning_best.h5',\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n\nprint(\"üöÄ MEMULAI TRAINING SIAMESE NETWORK...\")\nprint(f\"   Epochs: {EPOCHS}\")\nprint(f\"   Batch Size: {BATCH_SIZE}\")\nprint()\n\nhistory = siamese_model.fit(\n    [X1_train, X2_train], y_train,\n    validation_data=([X1_test, X2_test], y_test),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stop, reduce_lr, checkpoint],\n    verbose=1\n)\n\nprint(\"\\n‚úÖ Training Complete!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 9: TRAINING VISUALIZATION\n# =============================================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curve\naxes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\naxes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\naxes[0].set_title('Contrastive Loss During Training', fontsize=12)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Final metrics\nfinal_train_loss = history.history['loss'][-1]\nfinal_val_loss = history.history['val_loss'][-1]\nbest_val_loss = min(history.history['val_loss'])\n\nmetrics_text = f\"\"\"Training Summary:\n\nFinal Train Loss: {final_train_loss:.4f}\nFinal Val Loss: {final_val_loss:.4f}\nBest Val Loss: {best_val_loss:.4f}\n\nTotal Epochs: {len(history.history['loss'])}\n\"\"\"\n\naxes[1].text(0.5, 0.5, metrics_text, fontsize=14, \n             ha='center', va='center', family='monospace',\n             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\naxes[1].axis('off')\naxes[1].set_title('Training Summary', fontsize=12)\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 10: EVALUATION METRICS (AKADEMIS)\n# =============================================================================\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix, classification_report\n)\nfrom scipy.stats import pearsonr, spearmanr\n\n# Predict distances\nprint(\"üîÑ Generating predictions on test set...\")\npred_distances = siamese_model.predict([X1_test, X2_test], verbose=0).ravel()\n\n# Find optimal threshold using validation data\nthresholds = np.arange(0.1, 1.5, 0.05)\nbest_threshold = 0.5\nbest_f1 = 0\n\nfor thresh in thresholds:\n    y_pred_temp = (pred_distances < thresh).astype(int)\n    f1 = f1_score(y_test, y_pred_temp)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_threshold = thresh\n\nprint(f\"\\nüéØ Optimal Threshold (for classification): {best_threshold:.3f}\")\n\n# Apply optimal threshold\ny_pred = (pred_distances < best_threshold).astype(int)\n\n# Calculate metrics\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# ROC-AUC (perlu invert distance karena distance kecil = similar)\nroc_auc = roc_auc_score(y_test, -pred_distances)\n\n# Correlation dengan original scores\npearson_corr, _ = pearsonr(test_scores, -pred_distances)\nspearman_corr, _ = spearmanr(test_scores, -pred_distances)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üìä EVALUATION METRICS (TEST SET)\")\nprint(\"=\"*50)\nprint(f\"\\nüéØ Classification Metrics:\")\nprint(f\"   Accuracy:  {acc:.4f}\")\nprint(f\"   Precision: {prec:.4f}\")\nprint(f\"   Recall:    {rec:.4f}\")\nprint(f\"   F1-Score:  {f1:.4f}\")\nprint(f\"   ROC-AUC:   {roc_auc:.4f}\")\n\nprint(f\"\\nüìà Correlation with Ground Truth Scores:\")\nprint(f\"   Pearson:  {pearson_corr:.4f}\")\nprint(f\"   Spearman: {spearman_corr:.4f}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üìã Classification Report:\")\nprint(\"=\"*50)\nprint(classification_report(y_test, y_pred, target_names=['Different (0)', 'Similar (1)']))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 11: VISUALISASI EVALUASI\n# =============================================================================\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 1. Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n            xticklabels=['Pred Different', 'Pred Similar'],\n            yticklabels=['Actual Different', 'Actual Similar'])\naxes[0, 0].set_title('Confusion Matrix', fontsize=12)\naxes[0, 0].set_ylabel('Actual Label')\naxes[0, 0].set_xlabel('Predicted Label')\n\n# 2. ROC Curve\nfpr, tpr, _ = roc_curve(y_test, -pred_distances)\naxes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\naxes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\naxes[0, 1].set_title('ROC Curve', fontsize=12)\naxes[0, 1].set_xlabel('False Positive Rate')\naxes[0, 1].set_ylabel('True Positive Rate')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Distance Distribution\naxes[1, 0].hist(pred_distances[y_test == 1], bins=30, alpha=0.7, label='Similar pairs', color='green')\naxes[1, 0].hist(pred_distances[y_test == 0], bins=30, alpha=0.7, label='Different pairs', color='red')\naxes[1, 0].axvline(x=best_threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={best_threshold:.2f}')\naxes[1, 0].set_title('Distance Distribution by Label', fontsize=12)\naxes[1, 0].set_xlabel('Euclidean Distance')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].legend()\n\n# 4. Predicted Distance vs Ground Truth Score\naxes[1, 1].scatter(test_scores, pred_distances, alpha=0.5, s=10)\nz = np.polyfit(test_scores, pred_distances, 1)\np = np.poly1d(z)\naxes[1, 1].plot(test_scores, p(test_scores), \"r--\", linewidth=2, label=f'Trend line')\naxes[1, 1].set_title(f'Distance vs Ground Truth\\n(Pearson: {pearson_corr:.3f}, Spearman: {spearman_corr:.3f})', fontsize=12)\naxes[1, 1].set_xlabel('Ground Truth Score (0-5)')\naxes[1, 1].set_ylabel('Predicted Distance')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 12: DEMO SIMILARITY CHECK\n# =============================================================================\n\ndef check_similarity(text1, text2, threshold=None):\n    \"\"\"\n    Mengukur similarity antara dua teks menggunakan trained Siamese Network.\n    \"\"\"\n    if threshold is None:\n        threshold = best_threshold\n    \n    # Tokenize dan pad\n    seq1 = tokenizer.texts_to_sequences([text1])\n    seq2 = tokenizer.texts_to_sequences([text2])\n    pad1 = pad_sequences(seq1, maxlen=MAX_LEN, padding='post')\n    pad2 = pad_sequences(seq2, maxlen=MAX_LEN, padding='post')\n    \n    # Predict distance\n    distance = siamese_model.predict([pad1, pad2], verbose=0)[0][0]\n    \n    # Verdict\n    verdict = \"‚úÖ MIRIP\" if distance < threshold else \"‚ùå BEDA\"\n    confidence = max(0, min(100, (1 - distance/2) * 100))  # Rough confidence\n    \n    print(f\"A: {text1}\")\n    print(f\"B: {text2}\")\n    print(f\"Distance: {distance:.4f} (Threshold: {threshold:.2f})\")\n    print(f\"Verdict: {verdict} (Confidence: {confidence:.1f}%)\")\n    print()\n    \n    return distance\n\nprint(\"=\" * 60)\nprint(\"üß† DEMO: SIMILARITY METRIC LEARNING\")\nprint(\"=\" * 60 + \"\\n\")\n\n# Test Cases - IT & Brainstorming Context\nprint(\"--- KASUS 1: Paraphrase (Harusnya MIRIP) ---\")\ncheck_similarity(\"Login button is not working\", \"Cannot click the login button\")\n\nprint(\"--- KASUS 2: Same Topic (Harusnya MIRIP) ---\")\ncheck_similarity(\"Server returned 500 error\", \"Internal server error occurred\")\n\nprint(\"--- KASUS 3: Different Topics (Harusnya BEDA) ---\")\ncheck_similarity(\"Fix the login bug\", \"Order pizza for lunch\")\n\nprint(\"--- KASUS 4: Similar Structure, Different Meaning (TRICKY) ---\")\ncheck_similarity(\"How to learn Python?\", \"How to learn Java?\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 13: APLIKASI METRIC LEARNING UNTUK GROUPING IDE (IMPROVED)\n# =============================================================================\n\ndef group_ideas_with_metric_learning(ideas, similarity_threshold=None, show_analysis=True):\n    \"\"\"\n    Mengelompokkan ide berdasarkan similarity menggunakan trained Siamese Network.\n    \n    Algoritma:\n    1. Hitung pairwise distance untuk semua pasangan ide\n    2. Analisis distribusi distance untuk menentukan threshold optimal\n    3. Build similarity graph (edge jika distance < threshold)\n    4. Find connected components sebagai groups\n    \n    Ini BUKAN clustering algorithm terpisah - ini menggunakan hasil metric learning!\n    \"\"\"\n    n = len(ideas)\n    if n == 0:\n        return {}\n    \n    # Tokenize semua ide\n    seqs = tokenizer.texts_to_sequences(ideas)\n    padded = pad_sequences(seqs, maxlen=MAX_LEN, padding='post')\n    \n    # Hitung pairwise distances\n    print(f\"üîÑ Computing pairwise distances for {n} ideas...\")\n    distance_matrix = np.zeros((n, n))\n    all_distances = []\n    \n    for i in range(n):\n        for j in range(i+1, n):\n            dist = siamese_model.predict(\n                [padded[i:i+1], padded[j:j+1]], verbose=0\n            )[0][0]\n            distance_matrix[i, j] = dist\n            distance_matrix[j, i] = dist\n            all_distances.append(dist)\n    \n    all_distances = np.array(all_distances)\n    \n    # --- ANALISIS DISTANCE DISTRIBUTION ---\n    if show_analysis:\n        print(f\"\\nüìä ANALISIS DISTANCE:\")\n        print(f\"   Min distance: {all_distances.min():.4f}\")\n        print(f\"   Max distance: {all_distances.max():.4f}\")\n        print(f\"   Mean distance: {all_distances.mean():.4f}\")\n        print(f\"   Median distance: {np.median(all_distances):.4f}\")\n        print(f\"   Std deviation: {all_distances.std():.4f}\")\n    \n    # --- ADAPTIVE THRESHOLD FOR GROUPING ---\n    # PENTING: Threshold untuk grouping HARUS lebih ketat dari classification!\n    # Karena kita ingin hanya ide yang BENAR-BENAR mirip yang di-group\n    \n    if similarity_threshold is None:\n        # Gunakan percentile yang lebih ketat\n        percentile_25 = np.percentile(all_distances, 25)\n        percentile_10 = np.percentile(all_distances, 10)\n        percentile_5 = np.percentile(all_distances, 5)\n        \n        # Pilih threshold yang sangat ketat untuk grouping\n        # Hanya top 10-25% pasangan terdekat yang dianggap same group\n        if all_distances.mean() > 1.0:\n            # Distribusi distance tinggi - model kurang discriminative\n            similarity_threshold = percentile_5\n        elif all_distances.mean() > 0.7:\n            similarity_threshold = percentile_10\n        else:\n            similarity_threshold = percentile_25\n        \n        # HARD CAP - jangan pernah lebih dari 0.4 untuk grouping\n        similarity_threshold = min(similarity_threshold, 0.4)\n        \n        if show_analysis:\n            print(f\"\\nüéØ THRESHOLD SELECTION (GROUPING):\")\n            print(f\"   5th percentile:  {percentile_5:.4f}\")\n            print(f\"   10th percentile: {percentile_10:.4f}\")\n            print(f\"   25th percentile: {percentile_25:.4f}\")\n            print(f\"   ‚û°Ô∏è  Selected threshold: {similarity_threshold:.4f}\")\n            print(f\"   (Lebih ketat dari classification threshold!)\")\n    else:\n        if show_analysis:\n            print(f\"\\nüéØ Using manual threshold: {similarity_threshold:.4f}\")\n    \n    # Build adjacency based on threshold\n    adjacency = distance_matrix < similarity_threshold\n    np.fill_diagonal(adjacency, False)  # Jangan connect node ke dirinya sendiri\n    \n    # Find connected components (Union-Find style)\n    visited = [False] * n\n    groups = []\n    \n    def dfs(node, group):\n        visited[node] = True\n        group.append(node)\n        for neighbor in range(n):\n            if not visited[neighbor] and adjacency[node, neighbor]:\n                dfs(neighbor, group)\n    \n    for i in range(n):\n        if not visited[i]:\n            group = []\n            dfs(i, group)\n            groups.append(group)\n    \n    # Format output - pisahkan grouped dan ungrouped\n    result = {\n        \"groups\": {},\n        \"distance_matrix\": distance_matrix,\n        \"threshold_used\": similarity_threshold,\n        \"n_groups\": 0,\n        \"all_distances\": all_distances\n    }\n    \n    group_counter = 0\n    ungrouped_items = []\n    \n    for group in groups:\n        if len(group) > 1:  # Hanya grup dengan >1 member\n            group_counter += 1\n            group_name = f\"Group_{group_counter}\"\n            result[\"groups\"][group_name] = []\n            for i in group:\n                result[\"groups\"][group_name].append({\"index\": i, \"text\": ideas[i]})\n        else:\n            ungrouped_items.extend(group)\n    \n    result[\"n_groups\"] = group_counter\n    \n    # Tambahkan ungrouped items\n    if ungrouped_items:\n        result[\"groups\"][\"Ungrouped\"] = []\n        for i in ungrouped_items:\n            result[\"groups\"][\"Ungrouped\"].append({\"index\": i, \"text\": ideas[i]})\n    \n    return result\n\n# Demo\nprint(\"=\" * 60)\nprint(\"üéØ APLIKASI: GROUPING IDE DENGAN METRIC LEARNING\")\nprint(\"=\" * 60 + \"\\n\")\n\ntest_ideas = [\n    # Group 1: Login/Auth issues (sangat mirip)\n    \"Login button is broken\",\n    \"Cannot access my account\",\n    \"Password reset not working\",\n    \n    # Group 2: Server issues (sangat mirip)\n    \"Server returned 500 error\",\n    \"API is not responding\",\n    \n    # Group 3: UI changes (mungkin mirip)\n    \"Change the button color\",\n    \"Make the logo bigger\",\n    \n    # Outlier (harusnya tidak masuk grup manapun)\n    \"Order lunch for the team\"\n]\n\nresult = group_ideas_with_metric_learning(test_ideas)\n\nprint(f\"\\n\" + \"=\" * 40)\nprint(f\"üìä HASIL GROUPING: {result['n_groups']} groups found\")\nprint(f\"   (Threshold used: {result['threshold_used']:.4f})\")\nprint(\"=\" * 40 + \"\\n\")\n\nfor group_name, items in result[\"groups\"].items():\n    emoji = \"üîπ\" if group_name != \"Ungrouped\" else \"‚ö™\"\n    print(f\"{emoji} {group_name}:\")\n    for item in items:\n        print(f\"   [{item['index']}] {item['text']}\")\n    print()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 14: VISUALISASI DISTANCE MATRIX & THRESHOLD ANALYSIS\n# =============================================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# 1. Distance Matrix Heatmap\nax1 = axes[0]\nsns.heatmap(\n    result['distance_matrix'],\n    xticklabels=[f\"{i}\" for i in range(len(test_ideas))],\n    yticklabels=[idea[:20] + \"...\" if len(idea) > 20 else idea for idea in test_ideas],\n    cmap='RdYlGn_r',  # Reversed: hijau = jarak kecil (mirip)\n    annot=True,\n    fmt='.2f',\n    vmin=0,\n    ax=ax1\n)\nax1.set_title(f'Distance Matrix\\n(Threshold: {result[\"threshold_used\"]:.3f})', fontsize=12)\nax1.set_xlabel('Idea Index')\n\n# 2. Distance Distribution Histogram\nax2 = axes[1]\nax2.hist(result['all_distances'], bins=15, color='steelblue', edgecolor='black', alpha=0.7)\nax2.axvline(x=result['threshold_used'], color='red', linestyle='--', linewidth=2, \n            label=f'Grouping Threshold = {result[\"threshold_used\"]:.3f}')\nax2.axvline(x=best_threshold, color='orange', linestyle='--', linewidth=2,\n            label=f'Classification Threshold = {best_threshold:.3f}')\nax2.set_title('Distance Distribution (Pairwise)', fontsize=12)\nax2.set_xlabel('Euclidean Distance')\nax2.set_ylabel('Frequency')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° CATATAN:\")\nprint(\"   - Warna HIJAU di matrix = jarak KECIL (ide mirip)\")\nprint(\"   - Warna MERAH di matrix = jarak BESAR (ide beda)\")\nprint(f\"   - Pasangan dengan distance < {result['threshold_used']:.3f} akan di-group bersama\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 15: PENGUJIAN DOMAIN SPESIFIK (BRAINSTORMING)\n# =============================================================================\n\nprint(\"=\" * 60)\nprint(\"üìã PENGUJIAN OUT-OF-DOMAIN: STUDI KASUS JAMAL APP\")\nprint(\"=\" * 60 + \"\\n\")\n\n# Dataset test khusus brainstorming\njamal_test_data = [\n    # --- KATEGORI 1: IT & BUG REPORT ---\n    (\"Login button is broken\", \"Cannot click login\", 1),\n    (\"Server returned 500 error\", \"Internal server error\", 1),\n    (\"API response time is slow\", \"Latency is too high\", 1),\n    \n    # --- KATEGORI 2: UI/UX DESIGN ---\n    (\"Change color to blue\", \"Update background color\", 1),\n    (\"Make logo bigger\", \"Increase logo size\", 1),\n    \n    # --- KATEGORI 3: JEBAKAN (Topik Mirip tapi Beda) ---\n    (\"Fix login bug\", \"Design login page\", 0),\n    (\"Server is down\", \"Server is running fast\", 0),\n    \n    # --- KATEGORI 4: BEDA JAUH ---\n    (\"Server is down\", \"I want to eat pizza\", 0),\n    (\"Fix CSS style\", \"Meeting at 9 AM\", 0),\n    \n    # --- KATEGORI 5: PARAPHRASE SULIT ---\n    (\"I cannot remember my password\", \"Forgot password feature needed\", 1),\n]\n\n# Evaluate\ncorrect = 0\nresults_list = []\n\nfor text1, text2, expected in jamal_test_data:\n    seq1 = tokenizer.texts_to_sequences([text1])\n    seq2 = tokenizer.texts_to_sequences([text2])\n    pad1 = pad_sequences(seq1, maxlen=MAX_LEN, padding='post')\n    pad2 = pad_sequences(seq2, maxlen=MAX_LEN, padding='post')\n    \n    dist = siamese_model.predict([pad1, pad2], verbose=0)[0][0]\n    pred = 1 if dist < best_threshold else 0\n    \n    status = \"‚úÖ\" if pred == expected else \"‚ùå\"\n    correct += 1 if pred == expected else 0\n    \n    results_list.append({\n        \"text1\": text1,\n        \"text2\": text2,\n        \"expected\": expected,\n        \"predicted\": pred,\n        \"distance\": dist,\n        \"correct\": pred == expected\n    })\n    \n    expected_label = \"MIRIP\" if expected == 1 else \"BEDA\"\n    pred_label = \"MIRIP\" if pred == 1 else \"BEDA\"\n    \n    print(f\"{status} Expected: {expected_label}, Predicted: {pred_label} (d={dist:.3f})\")\n    print(f\"   A: {text1}\")\n    print(f\"   B: {text2}\\n\")\n\naccuracy = correct / len(jamal_test_data) * 100\nprint(f\"\\nüìä AKURASI PADA DOMAIN BRAINSTORMING: {accuracy:.1f}% ({correct}/{len(jamal_test_data)})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# CELL 16: TUNING THRESHOLD MANUAL (EKSPERIMEN)\n# =============================================================================\n\nprint(\"=\" * 60)\nprint(\"üîß EKSPERIMEN: TUNING THRESHOLD UNTUK GROUPING\")\nprint(\"=\" * 60 + \"\\n\")\n\n# Coba beberapa threshold berbeda\ntest_thresholds = [0.2, 0.3, 0.4, 0.5]\n\nfor thresh in test_thresholds:\n    print(f\"\\n--- THRESHOLD: {thresh} ---\")\n    result_exp = group_ideas_with_metric_learning(test_ideas, similarity_threshold=thresh, show_analysis=False)\n    \n    print(f\"Groups found: {result_exp['n_groups']}\")\n    for group_name, items in result_exp[\"groups\"].items():\n        if group_name != \"Ungrouped\":\n            item_indices = [item['index'] for item in items]\n            print(f\"   {group_name}: indices {item_indices}\")\n    \n    ungrouped = result_exp[\"groups\"].get(\"Ungrouped\", [])\n    if ungrouped:\n        print(f\"   Ungrouped: {len(ungrouped)} items\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## üìù KESIMPULAN\n\n### Algoritma yang Digunakan\n\n| Komponen | Detail |\n|----------|--------|\n| **Arsitektur** | Siamese Neural Network |\n| **Base Network** | Embedding + BiLSTM + Dense |\n| **Loss Function** | Contrastive Loss |\n| **Metric** | Euclidean Distance |\n| **Dataset** | STS Benchmark (Semantic Textual Similarity) |\n\n### Threshold untuk Grouping vs Classification\n\n| Task | Threshold | Keterangan |\n|------|-----------|------------|\n| Classification | `best_threshold` (~0.5-0.8) | Optimized untuk F1-Score |\n| Grouping | Much lower (~0.2-0.4) | Lebih ketat, hanya ide yang BENAR-BENAR mirip |\n\n### Hasil Eksperimen\n\n- Model berhasil belajar representasi semantik dari teks\n- Contrastive Loss efektif untuk mendorong pasangan mirip mendekat dan pasangan beda menjauh\n- **PENTING**: Threshold untuk grouping HARUS lebih ketat dari classification threshold\n- Hasil metric learning dapat diaplikasikan untuk grouping ide tanpa mengubah algoritma\n\n### Aplikasi untuk JAMAL\n\n1. **Input**: List of sticky notes dari canvas\n2. **Process**: Hitung pairwise distance menggunakan trained Siamese Network\n3. **Threshold Selection**: Gunakan percentile-based adaptive threshold\n4. **Output**: Grouping berdasarkan similarity threshold\n\n**Catatan Penting**: Grouping di sini BUKAN menggunakan algoritma clustering terpisah, melainkan memanfaatkan hasil dari **Similarity Metric Learning** untuk membangun graph similarity dan menemukan connected components.",
      "metadata": {}
    }
  ]
}
